
\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{array}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\geometry{margin=1in}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % Use a recent compatibility setting



\begin{document}

    

\begin{center}

    {\LARGE \textbf{Simplified Biostatistics - A Handbook}}\\[16pt]
    
   Denver JnBaptiste  
\newpage

\textbf{For Juden JnBaptiste}

\newpage

\section*{\underline{Table of Contents:}}

\qquad

% List of Topics
\section*{Frequency Distribution}
\section*{Class Mark}
\section*{Frequencies}
\section*{Mean}
\section*{Median}
\section*{Mode}
\section*{Sum of Squares}
\section*{Variance}
\section*{Sample variance (subset of population)}
\section*{Population variance}
\section*{{Standard deviation}
\section*{Binomial Distribution}
\section*{Normal Distribution}}
\section*{Confidence Intervals}
\section*{Confidence Limits}
\section*{Test Statistics}
\section*{Analysis of Variance (Anova)}
\section*{One way Anova}
\section*{Two way Anova}
\section*{Multiple Comparison test as part of Anova}
\section*{Student - Newman - Keul (SNK) Comparison}
\section*{Independent T-test}
\section*{Two-way Anova with replication}
\section*{Two way Anova without replication}
\section*{Paired t-test (Two way Anova)}
\section*{Homogeneity of Group Variances}
\section*{Wilcoxon}
\section*{Kruskal-Wallis H-test continuous data}
\section*{Regression Analysis}
\section*{Correlation Analysis}
\newpage

\section*{Chi-Square Goodness of Fit (Mendelian Genetics)}
\section*{Independence / Contingency table}
\section*{Relative Risk}
\section*{Principal Component Analysis}

\newpage

\end{center}

Data needs to be organized in a way that is descriptive. Statistics allows that. In this review, we will use various common techniques in statistics that will allow for simple analysis, examining discrete or continuous data. 

\newpage

\centering \section*{Frequency Distribution}

Frequency distribution helps analyze how often each value or group of values appears in your data. It organizes numbers into simple classes so you can easily spot patterns, trends, and make comparisons.

\qquad

Imagine the dataset below represents the heights of a specific type of plant. 
"Class Limits" of this plants height ranges from 30-99 centimeters (cm). 

\qquad

\centering \textbf{Table 1: Frequency Distribution Table Showing Class Limits and Class Boundaries- Plant Height Data}

\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
\textbf{Class Limits} & \textbf{Class Boundaries} \\
\midrule
30--39 & 29.5--39.5 \\
40--49 & 39.5--49.5 \\
50--59 & 49.5--59.5 \\
60--69 & 59.5--69.5 \\
70--79 & 69.5--79.5 \\
80--89 & 79.5--89.5 \\
90--99 & 89.5--99.5 \\
\bottomrule
\end{tabular}
\end{table}
Each group of the plant examined in the table above, falls within a specific range. The table represents these ranges. 

\noindent
\begin{minipage}[t]{0.48\textwidth}
\centering
\text{Lower boundry}
\vspace{1em}
\begin{align*}
LCB &= LL - \frac{1}{2} \text{ unit of measurement} \\
\\
LCB_1 &= 30 - \frac{1}{2}(1) \\
\\
&= 30 - \frac{1}{2} \\
&= 29\frac{1}{2} \\
\\
&= 29.5 \\
\\
LCB_2 &= 40 - \frac{1}{2}(1) \\
\\
&= 40 - \frac{1}{2} \\
\\
&= 39\frac{1}{2} \\
\\
&= 39.5
\\
\end{align*}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\text{Upper Boundary}
\vspace{1em}
\begin{align*}
UCB &= UL + \frac{1}{2} \text{ unit of measurement} \\
\\
UCB &= 39 + \frac{1}{2}(1) \\
\\
&= 39 + \frac{1}{2} \\
\\
&= 39\frac{1}{2} \\
\\
&= 39.5 \\
\\
LCB_2 &= 49 + \frac{1}{2}(1) \\
\\
&= 49 + \frac{1}{2} \\
\\
&= 49\frac{1}{2} \\
\\
&= 49.5
\end{align*}
\end{minipage}

Class boundaries help ensure that your data flows smoothly from one class to the next without leaving any gaps. They sit right between the upper limit of one class and the lower limit of the next. This makes the distribution continuous, so every data point has a clear spot to go, no overlaps, no confusion. It keeps histograms accurate and makes statistical work cleaner.

In the examples above, the unit of measurement for these plants is 1 (39-40, 49-50... etc. ) and so, we multiply this unit of measurement by 1/2 for the LCB and UCB respectively. You can substitute any unit of measurement based on your class limits to be multiplied by 1/2 for class boundaries. For example if the class limits were 35-40 and 45-50, then the unit of measurement would be 5. 

\qquad

We will continue with the same data set used for plant frequencies, lower and upper class boundaries, to analyze the class mark. 

\newpage 

\section*{Class Mark}

Class mark is important because it gives the center value of a class. It helps represent the whole class with one number, which makes calculations like mean and graphing easier and quicker.

\[
\text{Class Mark} = \frac{\text{Lower Class Limit} + \text{Upper Class Limit}}{2}
\\
\]

\centering \textbf{Table 2: Class Limits and Class Mark }
\begin{table}[H]
\centering
\begin{tabular}{c c}
\toprule
\textbf{Class Limits} & \textbf{Class Mark} \\
\midrule
30--39 & 34.5 \\
40--49 & 44.5 \\
50--59 & 54.5 \\
60--69 & 64.5 \\
70--79 & 74.5 \\
80--89 & 84.5 \\
90--99 & 94.5 \\
\bottomrule
\end{tabular}
\end{table}

\text{CM}_1 = \frac{30+39}{2}
= 34.5

\qquad

\\
\text{CM}_2 = \frac{40+49}{2}
= 44.5 

\qquad

A histogram is a bar graph (representing discrete data) that has continuous data.

\qquad

Take a look at the histogram below, which represents our table so far: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Screen Shot 2025-11-18 at 10.30.01 PM.png}
    \caption{Histogram representing the \textbf{class boundaries-continuous data points} and class \textbf{mark} for data above.}
    \label{fig:placeholder}
\end{figure}

\qquad

The height of each of each bar in our histogram is proportional to frequencies. The widths here represent class boundaries. The data above represent continuous data points. If the bars for the (frequency) histogram are removed, you are left with specific points (class marks) that represents a frequency polygon. It's the same plant data, just represented differently. 

\newpage

\section*{Frequencies}

\textbf{Table 3: Frequency Distribution Table}

\qquad

Plant Height Data. 

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\toprule
\textbf{Frequency} & \textbf{Relative Freq.} & \textbf{Cumulative Freq.} & \textbf{Relative Cum. Freq.} \\
\midrule
2 & 2/60 & 2 & 2/60 \\
12 & 12/60 & 14 & 14/60 \\
16 & 16/60 & 30 & 30/60 \\
6 & 6/60 & 36 & 36/60 \\
8 & 8/60 & 44 & 44/60 \\
9 & 9/60 & 53 & 53/60 \\
7 & 7/60 & 60 & 60/60 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{The table above shows frequency distribution with Relative Frequencies, Cumulative Frequencies and Relative Cumulative Frequencies. 
}

\qquad

Relative frequency shows how often something happens in proportion to the total. It’s a percentage or decimal of the total. So, plant height #1 is 2 cm with respect to the total height of the plant which is 60cm. The same follows for the other heights in our table above. 

\qquad

Cumulative Frequency keeps adding up the frequencies as you move down the table. It shows how many values fall up to that point.

\qquad

This analysis is good for comparison of various samples. In our case here, it's the height of plants. 

\qquad

Relative Cumulative Frequency is the same as cumulative, but in percentage form. It tells you what percent of data falls up to that point.

\newpage

\begin{table}[h!]
    \centering \textbf{Table 4: Frequency Distribution- Table for Central Tendency}

\qquad
    
    \begin{tabular}{@{}ccccr@{}}
    \toprule
    \textbf{Class Limits} & \textbf{Frequency ($y$)} & \textbf{Class mark ($x$)} & \textbf{$fx$} \\
    \midrule
    30--39 & 2 & 34.5 & 69 \\
    40--49 & 12 & 44.5 & 534 \\
    50--59 & 16 & 54.5 & 872 \\
    60--69 & 6 & 64.5 & 387 \\
    70--79 & 8 & 74.5 & 596 \\
    80--89 & 9 & 84.5 & 760.5 \\
    90--99 & 7 & 94.5 & 661.5 \\
    \midrule
    \textbf{Total} & \textbf{60} & & \textbf{3880.0} \\
    \bottomrule
    \end{tabular}
\end{table}

The table above demonstrates the values needed to calculate the measures of central tendency (mean, median and mode). 

\newpage

\section*{Mean}

Mean tells you the typical or central value of your data. To find the mean, simply add up all the numbers and divide by the total of the numbers (this is the simple arithmatic mean). 

\qquad

We are going to calculate the frequency sample mean (weighted mean for grouped data) of the data presented in our tables so far. 

\qquad

For group data, each value is multiplied by its respective frequency, the products are added up and divided by the total frequency. 

\qquad

So: 


\text{Population mean} \Rightarrow \mu = \frac{\sum x_i}{\sum f}

\qquad

\text{Sample mean} \Rightarrow \bar{x} = \frac{\sum x_i}{\sum f}

\qquad

\noindent\fbox{%
    \parbox{\textwidth}{%
        \text{Frequency Sample mean} \Rightarrow \bar{x} = \frac{\sum f_i x_i}{\sum f}.
    }%
}

\qquad

$\bar{x} = \frac{\sum f_i x_i}{\sum f}$

\qquad

$\bar{x} = \frac{3880}{60} = 64.67$.

\newpage

\section*{Median}

Median is the middle value when the numbers are arranged from smallest to largest. This is especially useful to show the center of data when there are outliers. 

\qquad

If the frequency total of the data is \textbf{odd}, use:
$$\frac{n+1}{2}$$

If frequency total is \textbf{even} use:
$$\frac{\frac{n}{2}+(\frac{n}{2}+1)}{2}$$

After you get the solution to the calculations (\textbf{odd} or \textbf{even} data values), simply look in the \textbf{cumulative frequency} section, for the median cumulative frequency and frequency total from our plant data table and that would be your solution.

\qquad

For frequency tables where there are class limits, use:
$$L+\left(\frac{\frac{\Sigma f}{2}-B}{G}\right)\times C$$

where \\

\raggedright 

* L = lower class limit of median group frequency based on cumulative frequency. \\

\qquad

* $\Sigma f$ = frequency total. \\

\qquad

* B = cumulative frequency of group before median group \\

\qquad

* G = frequency of the median group \\

\qquad

* C = group width.

\qquad

\\[2\baselineskip]

\centering

So, the median:

\qquad

Since $\sum f = 60$ (even). $\Rightarrow \frac{n}{2}$

\qquad

$\frac{60}{2} = 30$.

\qquad

So based on the cumulative frequency, our lower class limit is 49.5. 

\qquad

$L = 29.5$

\qquad

so median $= 49.5 + \left(\frac{\frac{60}{2} - 14}{16}\right) \times 10$.

\qquad

$= 49.5 + \left(\frac{30 - 14}{16}\right) \times 10$.

\qquad

$= 49.5 + \left(\frac{16}{16}\right) \times 10$.

\qquad

$= 49.5 + 10$.

\qquad

$= 59.5$.

\qquad

So median group is $60-69$, based on class boundaries (table 1). 

\qquad


\newpage

\section*{Mode}

\qquad

Mode is the value that appears most often in the data (common or frequent value). 

\qquad

From frequency distributions, the mode can be calculated using the equation below:

\\[2\baselineskip]

Mode: $L + \left(\frac{\Delta_1}{\Delta_1 + \Delta_2}\right) \times C$.

\qquad

$L$ = lower class boundary of number with the most freq.

\qquad

$\Delta_1$ = number in frequency with the most freq before it...

\qquad

$\Delta_2$ = number in freq with 2nd most freq before it...

\qquad

$C$ = distance of limit or boundary/width.

\qquad

\vspace{1em} % Add some vertical space

$\therefore$ Mode $= 49.5 + \left(\frac{(16-12)}{(16-12)+(12-2)}\right) \times 10$

\begin{align*}
&= 49.5 + \left(\frac{4}{4+10}\right) \times 10 \\

\qquad

&= 49.5 + \left(\frac{4}{14}\right) \times 10 \\

\qquad

\displaystyle{\displaylines{&= 49.5 + \frac{20}{7} \\}}

\qquad

&= 52.357 is our mode

\qquad

There may be plants that appear to have the same heights based on condition/ conditions that are similar. 

\qquad

\end{align*}


\newpage

\section*{Sum of Squares}

\qquad

Let's use our plant height tables to examine the sum of squares. 

\qquad
The Sum of Squares is the total of all squared differences from the mean. This can show how spread out the data is. \textbf{So, the bigger the sum of squares, the more spread out your data }

\qquad

The sum of squares is also the base for some other statistical outlooks such as variance, standard deviation, ANOVA, regression principle component analysis and more. Which we will discuss later. 

\qquad

The equation for sum of squares is seen below: 


$$SS = \Sigma fx^2 - \frac{(\Sigma fx)^2}{\Sigma f}$$

\vspace{1cm}

\centering \textbf{Table 5: Frequency Distribution with $x^2,  fx^2$ values}

\qquad

\begin{tabular}{ccccc}
\toprule
Class mark ($x$) & frequency ($f$) & $x^2 & fx^2$ & $fx$ \\
\midrule
34.5 & 2 & 1190.25 & 2380.5 & 69 \\
44.5 & 12 & 1980.25 & 23763 & 534 \\
54.5 & 16 & 2970.25 & 47524 & 872 \\
64.5 & 6 & 4160.25 & 249615 & 387 \\
74.5 & 8 & 5550.25 & 44402 & 596\\
84.5 & 9 & 7140.25 & 64262.25 & 760.5 \\
94.5 & 7 & 8930.25 & 62511.75 & 661.5 \\
\midrule
Total & 60 & & 269805 & 3880 \\
\bottomrule
\end{tabular}

\qquad

Table showing the set up for sum of squares calculations.

\vspace{1cm}

$$SS = 269805 - \frac{(3880)^2}{60}$$

\qquad

$$= 269805 - \frac{15054400}{60}$$

\qquad

$$= 269805 - 250906.67$$

\qquad

$$= 18898.33$$

This is our sum of squares for plant height or the spread of our data. The sum of spuares for one sample set can be compared to another sample set. 

\newpage

\section*{Variance:}

\qquad

Variance shows how spread out the data is from the average (how many data points vary or deviate from the center). 

\qquad

So: If all values, for height for example, are close to the mean → low variance (data is consistent).

If values are far from the mean → high variance (data is spread out).
The same applies to sample and population variance. 

\subsection*{Sample variance (subset of population):}

$$S^{2} = \frac{\sum f(x)^{2} - \frac{(\sum fx)^{2}}{\sum f}}{\sum f - 1}$$

notice sample variance $\Rightarrow \frac{SS}{\sum f-1}$

\subsection*{Population variance:}

$$\sigma^{2} = \frac{\sum(x_{i}-\mu)^{2}}{N}$$

so, in our problem:

\qquad

Sample variance $= \frac{18898.33}{60-1}$

\qquad

$S^{2} = 320.31$

\qquad

Population variance = $\sigma^{2}=\frac{\sum(x_{i}-\mu)^{2}}{n}

\qquad



&= \frac{
(34.5-64.67)^{2} + (44.5-64.67)^{2} + (54.5-64.67)^{2} +{}\\
&\qquad
(64.5-64.67)^{2} + (74.5-64.67)^{2} + {}\\
&\qquad
(84.5-64.67)^{2} + (94.5-64.67)^{2}}{60}

\qquad

The denominator is 60 and our solution is 46.68. That is our population variance. 


\newpage

\section*{Standard deviation}

\qquad

Standard deviation is a measure that shows how spread out the values in a dataset are from the mean (average).

\qquad


If the standard deviation is small → the data points are close to the mean (less variation).

\qquad

If the standard deviation is large → the data points are far from the mean (more variation).

\\[2\baselineskip]

Let's use out plant height data set to find the standard deviation. 

The formula for standard deviation is below: 

\qquad

$S = \sqrt{S^2} \Rightarrow \text{Sample standard deviation}$

\qquad

so, the square root of variance is the standard deviation. 

\qquad

Below is how we calculate standard deviation:

\qquad

$\sigma = \sqrt{\sigma^2} \Rightarrow \text{Population standard deviation}$

\qquad

$\therefore \text{Since } S^2 = 320.31$

\qquad

$S = \sqrt{320.31}$

\qquad

$= 17.9$
is our standard deviation (SD).

\qquad

If we want to calculate the Coefficient of variation $\Rightarrow V$

\qquad

This is the analysis of relative dispersion of the data. The SD is divided by the mean. 

\qquad

$V = \frac{S}{\bar{x}} \times 100 \%$

\qquad

$\therefore V = \frac{17.9}{64.67} \times 100$

\qquad

$= 0.277 \times 100$

\qquad

$= 27.7 \%$

\newpage

\section*{Binomial Distribution}

\qquad

If you want to use binomial distribution to analyze data, (counting how many times something succeeds out of a fixed number of trials), you need the following features:


\qquad

\raggedright

* Each trial has two outcomes, this deals with success or failure, head/tail, pass/fail etc.

\qquad

* The number of trials (n) is fixed.

\qquad

* The probability of success (p) stays the same each time.

\qquad

* All trials are independent, one outcome doesn't affect the next.

\qquad

Recall from probability and statistics: (1-q) = p where p is the success rate and q is the rate of failure. 

\vspace{1cm}

\centering

\begin{tabular}{|l|c|}

\hline
Class limits & Frequency \\
\hline
30-39 & 2 \\
40-49 & 12 \\
50-59 & 16 \\
60-69 & 6 \\
70-79 & 8 \\
80-89 & 9 \\
90-99 & 7 \\
\hline
Total & 60 \\
\hline
\end{tabular}
\hfill

\vspace{1cm}

If we are dealing with success or failure, from our plant height dataset:  

\qquad

A) $P(x \ge 60) = \frac{6+8+9+7}{60} = \frac{30}{60} = 0.5$

\qquad

B) What if, out of our data, a sample of 6 students was selected; what is the probability of all 6 scoring below 60.

\qquad

Since $P(x \ge 60) = 0.5$, then $P(x < 60) = 1 - 0.5 = 0.5$.

\qquad

Therefore, the probability of all 6 scoring below 60 is $P^6 = (0.5)^6 = 0.016$.

\qquad

c) The probability of the first 4 scoring 60 or more.

$$\binom{6}{4} (p^4 q^2) = \binom{6}{4} (0.5)^4 (0.5)^2$$
$$= 15(0.0156)$$
$$= 0.234$$ is the probability of the first 4 scoring 60 or more. 

Other useful formulas for Binomial Distribution:

\qquad

mean $\Rightarrow \bar{x} = np$
variance $\Rightarrow s^2 = npq$
Standard deviation $\Rightarrow s = \sqrt{npq}$
 where n represents the total number of trials. 


\newpage

\section*{Poisson Distribution}

Poisson Poisson distribution shows how many times an event is likely to happen in a fixed time or space, when:

\qquad

\raggedright

* The event happens randomly.

\qquad

* It happens independently (one does not affect another).

\qquad

* It happens at an average known rate lambda (\lambda.

\qquad

The formula for Poisson distribution is: 

$$P(x)=\frac{\mu^{x}}{e^{\mu}x!}$$

\centering

\qquad

where $\mu$ is the Parameter of the distribution.

\qquad

Poisson distribution approaches the binomial distribution where $n$ is large and $p$ is very small.

\qquad

$\sigma^2$ in poisson distribution also $= \mu$; in Binomial they are different.

\qquad

Poisson can be used if $p$ is very small, $N$ is $\ge 30$.

\qquad

If $N$ is small, Binomial is used.

\qquad

eg. If there are 90 cars and $2\%$ normally have defective bolts. What is the probability that 6 bolts are found defective?

\qquad

So: $n=90$

\qquad

$2\%=0.02$.

\qquad

So: $\mu=(90)(0.02)$
$= 1.8$

\qquad

So: $P(x) = \frac{\mu^x}{e^\mu x!} = \frac{1.8^6}{e^{1.8}(6!)} = \frac{1.8^6}{4.355.7} = 0.0078...$

\qquad

P(X$<2) \text{ are defective: } = P(X=1)+P(X=0)
$$
=\frac{1.8^{1}}{e^{1.8}(1!)}+\frac{1.8^{0}}{e^{1.8}(0!)}
$$
$$
= 0.298 + 0.165
$$
$$
= 0.463
\qquad

$$
$P(X\ge 2)=1-\{P(X<2)\}$
$$
= 1-0.463
$$
$$
\qquad

so, the probability that 6 bolts are defective is: 
= 0.537

$$

\qquad

Coefficient of dispersion  (CD) is found when Poisson is concerned, it checks for randomness for Poisson because \mu 

and variance = 1.

\qquad

$$
CD=\frac{s^{2}}{\overline{x}}.
$$

\newpage

\section*{Normal Distribution}

Normal distribution is \textbf{symmetric around the mean,} where most values cluster near the mean and fewer values occur as you move farther away.

\qquad

For normal distribution:

\begin{itemize}
    \item mean = median = mode
    \item random \& independent
    \item expected value = mean
    \item variance = 1, standard deviation = 1
    \item standard score/deviate = Z-score
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{Screen Shot 2025-11-20 at 8.24.15 PM.png}
    \textbf{\caption{Bell Shaped Probability Distribution- Normal Distribution}}
\end{figure}

$$Z = \frac{x_i - \mu}{\sigma}$$

\qquad

Let's use some arbiturary values here:

\qquad

If $\mu = 64.67$ mm and we are asked what proportion of the population is greater than 67 mm, on our bell curve, since the standard score or deviate \rightarrow z score. 

$$Z = \frac{67 - 64.67}{17.9}$$

$$= 0.130$$

So: $P(x_i > 67\text{mm}) = P(Z > 0.13) = 0.5517$. from the \textbf{normal distribution table}.

for two tailed = 0.5517. \\

\qquad

for one tailed = 0.4483 \\

\qquad

So: $1 - 0.4483$ \\

\qquad

$= 0.5517$  \\
is the proportion of the population greater than 67


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{image.png}
    \caption{Two tailed distribution}
\end{figure}

\newpage

\section*{Confidence Intervals}

\qquad

Confidence intervals are range values, calculated from sample data. 
CI's show how sure we are, about an estimate made. CI's are normally represented as percentages (95 \%). 

\qquad

Formulas for CI's: 

\qquad

\raggedright

1) $P\left(-t_{\alpha v} \le \frac{\bar{x} - \mu}{s_{\bar{x}}} \le t_{\alpha v}\right) \quad 1-\alpha$

\qquad

2) $P\left(-Z \le \frac{\bar{x} - \mu}{\sigma_{\bar{x}}} \le Z\right)$

\qquad

3) $\frac{vs^2}{\chi^2_{\alpha/2, v}} \le \sigma^2 \le \frac{vs^2}{\chi^2_{1-\alpha/2, v}}$

\qquad

where \alpha; 

\hspace{represents the significance level:} 

\qquad

95 \% \Rightarrow 
\alpha = 0.05, 

\qquad

90\% 
\Rightarrow 0.1, 

\qquad

99\% \Rightarrow 0.01

\newpage

\centering

\section*{Confidence Limits}

\qquad

Confidence limits are lower and upper boundaries of CI's

$L_{1} = \bar{x} - z\sigma_{\bar{x}}$ 

\qquad

$L_{2} = \bar{x} + z\sigma_{\bar{x}}$

\qquad

for $Z$, the same principle of upper and lower limits can be applied for $t$ \& $\chi^2$ (chi square).

\section*{Test Statistics}

\qquad

Test statistics are numbers calculated from sample data that are used to decide whether to reject or accept a null hypothesis in a statistical test.

\qquad

Test statistics should not be confused with the statistics tests which we have been doing. Test statistics summarize the difference between what you observe in your data and what you would expect under the null hypothesis, often in terms of standard errors.

\qquad

Examples of test statistics include t, z, chi-square, and F values.

\qquad

\begin{enumerate}
    \item $t = \frac{\bar{x} - \mu}{s_{\bar{x}}}$ \quad if $\sigma$ is unknown, $n < 30$
    \item $Z = \frac{\bar{x} - \mu}{\sigma_{\bar{x}}}$ \quad if $\sigma$ is known, $n \ge 30$
    \item $\chi^2 = \frac{s^2v}{\sigma^2}$ Skewed to the right (chi square).
\end{enumerate}

\qquad

Where probability distribution is skewed to the right, higher values are less likely. 

\qquad

For any test statistic, the null is accepted if there is no significant difference between the observed and the expected. If the null is rejected, the alternative is true and there is significant difference between the observed and the expected. 

\newpage

\section*{Analysis of Variance (Anova)}

\qquad

Anova is a statistical test, which compares the means of three or more groups to see if at least one group mean is significantly different from the other group means.

It works by analyzing the variance within groups versus the variance between groups.

\subsection*{One way Anova:}
\begin{enumerate}
    \item Fixed
    \item Random
\end{enumerate}
*fixed = specific levels of interest, random = sampled from a larger population

\qquad

$\implies$ only 1 null hypothesis is needed for one way Anova

\subsection*{Two way Anova:}
\begin{enumerate}
    \item without replication (2 null hypotheses)
    \item with replication (3 null hypotheses)
\end{enumerate}

One Way Anova determines whether there is any statistical significance between means of three or more groups that are not related.

So $H_{0} = \mu_{1} = \mu_{2} = \mu_{3} \dots \mu_{k}$

\qquad

Let's look at an example of one way Anova using our "Mice table" below (Mice grown in condition "X", "Y", "Z" ).

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Mice & X & Y & Z \\
        \hline
        & 8 & 6 & 8 \\
        & 9 & 2 & 9 \\
        & 5 & 3 & 7 \\
        & 3 & 4 & 5 \\
        \hline
        Total & 25 & 15 & 29 \\
        \hline
    \end{tabular}
\end{table} \\
\caption{Table showing Mice Grown in Condition "X", "Y", "Z" }

\qquad

$\text{mean} = x 6.25 \quad y3.75 \quad z7.25$

$\text{SD} = x2.75 \quad y1.71 \quad z1.71$

$\text{Grand total} = 69$

\begin{table}[h]
    \centering

    \begin{tabular}{|l|c|c|c|c|c|}
        \hline
        Source of variance & SS & df & MS & $\text{F}_{S}$ & $\text{F}_{\text{table}}$ \\
        \hline
        Among Mice & 26 & 2 & 13 & 2.91 & $\text{F}_{0.05}(2,9)$ \\
        within Mice & 40.25 & 9 & 4.47 & & $= 4.26$ \\
        \hline
        Total Mice & 66.25 & 11 & & & \\
        \hline
    \end{tabular}
\end{table}

from the "F" table for test statistics: If $\text{F}_{S} > \text{F}_{\text{table}}$ value, then reject the null, there is no significant difference.

\qquad

\begin{align*}
\text{SS among} &= \frac{(\sum A)^2}{n} - \frac{(\text{Grand total})^2}{N} \\
&= \frac{2S^2 + 1S^2 + 2q^2}{4} - \frac{(6q)^2}{12} \\

\displaystyle{\displaylines{&= \frac{1691}{4} - \frac{4761}{12} \\}}

\qquad

&= 563.67 - 396.75 \\

\qquad

&= 26.
\end{align*}
\begin{align*}
\text{Total SS} &= \sum A^2 - \frac{(\text{Grand total})^2}{N} \\
&= \left( (8^2 + 9^2 + 5^2 + 3^2) + (6^2 + 2^2 + 3^2 + 4^2) + (8^2 + 9^2 + 2^2 + 5^2) \right) - 396.75 \\
&= 463 - 396.75 \\
&= 66.25
\end{align*}

\begin{align*}
\text{Within SS} &= \text{Total SS} - \text{Among SS} \\
&= 66.25 - 26 \\
&= 40.25
\end{align*}

Where SS = Sum of Squares

\newpage

\section*{Multiple Comparison test as part of Anova}

\qquad

The Multiple Comparison Test (also called post-hoc test) is done after ANOVA if you find a significant difference, this helps determine which specific group means are different from each other.

\qquad

Common methods include:

\raggedright

* Tukey’s HSD (Honestly Significant Difference) – controls overall error rate

\qquad

* Bonferroni – adjusts for multiple comparisons

\qquad

* Scheffé – more conservative

\qquad

So, ANOVA tells you there is a difference somewhere in the represented data; multiple comparison tests tell you exactly where you can find that difference.

\qquad

\centering

\textbf{Least Significant Difference (LSD)}

\qquad

The Least Significant Difference (LSD) is a type of multiple comparison (post-hoc) test.

\qquad

It compares pairwise differences between group means.

\qquad

If the difference between two means is greater than the LSD value, it’s considered statistically significant.

\qquad

LSD has a higher chance of \textbf{Type I error} (reject the null hypothesis even though it is actually true - you think there is an effect when there is actually no effect) when many comparisons are made because it is not very conservative. 

\qquad

*Since we are on this topic or errors: False Negatives are type two errors: Failing to reject the null hypothesis when it is false.

\qquad

BACK TO LSD:

\qquad

From our "Mice" data table: 

$$ \text{LSD} = t_{\alpha(\text{df})} \sqrt{MS_w \left(\frac{1}{n_1} + \frac{1}{n_2}\right)} $$

Control vs y

\qquad

Control vs x

\qquad

Control vs z

\qquad

means:
Control = 6,
y = 3.75,
x = 6.25,
z= 7.25     

Since x, y, \& z has the same n(4) we can use the same LSD for each comparison.

$$ \text{LSD} = t_{0.05(9)} \sqrt{4.47 \left(\frac{1}{4} + \frac{1}{4}\right)} $$

$$ = 2.262 \sqrt{4.47 \left(\frac{2}{4}\right)} $$

$$ = 2.262 (1.49) $$

$$ = 3.37 $$

For control vs y:
$$ |6 - 3.75| = 2.25 $$

$$ 2.25 < \text{LSD} $$

Therefore, accept $H_0$.

\newpage

\section*{Student - Newman - Keul (SNK) Comparison}

\qquad

This is another post-hoc test, used after ANOVA and helps identify which group means are significantly different.

\qquad

It orders group means from smallest to largest.

\qquad

Compares all pairs stepwise, starting with the largest difference.

\qquad

Less conservative than Tukey’s HSD, so it has a higher chance of Type I error.

\qquad

Useful when you want more power to detect differences but accept slightly higher error risk.

Helps find which group means differ after ANOVA

\qquad

\textbf{\caption{From the Mice data table, we have the table below where SE is standard Error, q is the calculated q-value and Q is the critical q value. So, the null is accepted when the calculated q value is less than the critical q value}:
}
\begin{tabular}{l|l|l|l|l|l|l}
\toprule
Comparison & Difference & SE & P-range & $q = \frac{\text{Diff}}{\text{SE}}$ & $Q(2, 9)$ (from table) & Decision \\
\midrule
x vs y & $6.25 - 3.75$ & $\frac{\sqrt{ms_w}}{\sqrt{n}}$ & 2 & $\frac{2.5}{1.06}$ & 3.199 & accept $H_0$ \\
& $= 2.5$ & $= \sqrt{\frac{4.47}{4}}$ & & $= 2.36$ & & \\
& & $= 1.06$ & & & & \\
\bottomrule
\end{tabular}

\newpage

\section*{Independent T-test}

\qquad

The data for independent T-tests is assumed to be normally distributed and has similar variances

The Independent T-test is also called two-sample t-test. It is used to  compare the means of two independent groups to see if they are significantly different.

\qquad

\caption{Let's look at the table below with students in a class:}

\qquad

\vspace{0.5cm}

\begin{tabular}{lcc}
\textbf{Boys} & \textbf{Girls} & \\
8 & 5 & \\
3 & 4 & \\
2 & 3 & \\
8 & 2 & \\
9 & 8 & \\
10 & 10 \\
\hline
40 & 32 \\

\end{tabular}

\qquad

Formula: 

\qquad

$t_{s} = \frac{\bar{x}_{1} - \bar{x}_{2}}{\sqrt{\frac{s_{1}^{2}}{n_{1}} + \frac{s_{2}^{2}}{n_{2}}}}$

\vspace{0.5cm}

$\bar{x}_{1} = \frac{40}{6} = 6.67$

\qquad

$s_{1}^{2} = 11.07$

\qquad

$\bar{x}_{2} = \frac{32}{6} = 5.33$

\qquad

$s_{2}^{2} = 9.47$

\vspace{0.5cm}

So: $t_{s} = \frac{6.67 - 5.33}{\sqrt{\frac{11.07}{6} + \frac{9.47}{6}}}$

\qquad

$= \frac{1.34}{\sqrt{3.42}}$

\qquad

$= 0.72$

\vspace{0.5cm}

$t_{0.05((n_{1}+n_{2})-2)}$

\qquad

$t_{0.05(10)} = 2.228$

\qquad

So $t_{s} < t_{\text{table}} \Rightarrow \text{accept } H_{0}$.

\qquad

There is no significant difference between the two groups.

\newpage

\section*{Two-way Anova with replication}

\qquad

Recall: Anova compares the means of three or more groups to see if at least one group mean is
significantly different from the other group means.

\qquad

Two way Anova examines the effect of two independent factors on a dependent variable, allows examination of whether there is an interaction between the factors.

\qquad

\textbf{\caption{Mean Squares (MS) for Different ANOVA Models}}
\begin{table}[h]
    \centering
    \label{tab:anova_ms}
    \begin{tabular}{lccc}
        \toprule
        & Fixed A, Fixed B (MS) & Random A, Random B (MS) & Fixed A, Random B (mixed) (MS) \\
        \midrule
        \textbf{Factor A} & $\frac{\text{Factor A}}{\text{Error}}$ & $\frac{\text{Factor A}}{\text{Intersect}}$ & $\frac{\text{Factor A}}{\text{Intersection (A}\times\text{B)}}$ \\
        \textbf{Factor B} & $\frac{\text{Factor B}}{\text{Error}}$ & $\frac{\text{Factor B}}{\text{Intersect}}$ & $\frac{\text{Factor B}}{\text{Error}}$ \\
        \textbf{Intersect A}\times\textbf{B} & $\frac{\text{A}\times\text{B}}{\text{Error}}$ & $\frac{\text{A}\times\text{B}}{\text{Error}}$ & $\frac{\text{A}\times\text{B}}{\text{Error}}$ \\
        \bottomrule
    \end{tabular}
\end{table}

Examples from the tables below: 

\qquad

Lions by Country Data

\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Country} & \multicolumn{3}{|c|}{\textbf{Lions}} & \textbf{Total} \\
\cline{2-4}
 & \textbf{I} & \textbf{II} & \textbf{III} & \\
\hline
African & 2 & 4 & 5 & \\
African & 8 & 2 & 3 & \\
Subtotal & 10 & 6 & 8 $ $ & 24\\
\hline
Other & 6 & 7 & 8 & \\
Other & 5 & 2 & 4 & \\
Subtotal & 11 & 9 & 12 $ $ & 32 \\
\hline
Column total & 21 & 15 & 20 & \textbf{Grand total = 56} \\
\hline
\end{tabular}

\newpage

\caption{F values for \alpha= 0.05}

\begin{table}[h]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        Source of variation & SS & DF & MS & F & $F_{\text{crit}}$ \\
        \midrule
        Among Lions & 5.17 & 2 & 2.86 & 0.025 & $F_{(2,7)}=4.74$ \\
        Between Country & 5.34 & 1 & 5.34 & 0.047 & $F_{(1,7)}=5.59$ \\
        Lions $\times$ Country (A$\times$B) & 1.16 & 2 & 0.58 & 0.005 & $F_{(2,7)}=4.74$ \\
        Error & 793 & 7 & 113.3 & & \\
        Total & 804.67 & 11 & & & \\
        \bottomrule
    \end{tabular}
    
\end{table}
\label{Table showing }

\text{accept } H_{0}  since F statistics is less than  F critical.

\qquad

\qquad

SS: sum of squares
DF : Degree of Freedom (n-1)
MS: means square 
F: is F statistic from the table.
The( \displaystyle{\displaylines{\frac{MS}{Error}}})

\qquad

ratio of variance between groups to variance within groups. A large F-statistic suggests significant difference between the groups. 

 
F_{\text{crit}: 

F-distribution based on the degree of freedom and significance level. 

\newpage

Total\ SS = \Sigma A^{2} - \frac{(\Sigma A)^{2}}{N}

\qquad

= (21^{2}+15^{2}+20^{2}) - \frac{56^{2}}{12}
= 1066 - 261.33
= 804.67. \approx

\qquad

\therefore Error = Total - (among + Between +(A \times b))$

\qquad

= 804.67 - (5.17+5.34+1.16)

\qquad

= 793 is the value for sum of squares for Error
\newpage

\section*{Two way ANOVA cont...}

$$SS_{\text{Among}} = \frac{\Sigma A^2}{n} - \frac{(\text{Grand total})^2}{N}$$

\noindent Among Lions:
$$SS_{\text{Among}} = \frac{21^2 + 15^2 + 20^2 - (56)^2}{4}$$
$$= 266.5 - 261.33$$
$$= 5.17$$

\noindent Between country:
$$\frac{24^2 + 32^2 - 3136}{6}$$
$$= 266.67 - 261.33$$
$$= 5.34$$

\noindent (Lions X country (intersection)) SS:
$$\text{Individual cells} \Rightarrow \frac{10^2 + 6^2 + 8^2 + 11^2 + 9^2 + 12^2}{2} - \frac{3136}{12}$$
$$= 273 - 261.33$$
$$= 11.67$$

\noindent Lion X country SS = Individual cells - (Among SS + Between SS)
$$= 11.67 - (5.17 + 5.34)$$
$$= 1.16$$

\newpage

\section*{Two way Anova without replication}

\qquad

No interaction is tested with two way Anova without replication because there is only one observation.

\qquad

\textbf{\caption{Anova Table:}}
\begin{table}[h!]
    \centering
    \begin{tabular}{l c c c c c c}
        \toprule
        Source of Variation & S.S & Df & MS & F$_{S.0.05}$ & F$_{table}$ & Decision \\
        \midrule
        Among Region (A) & 8.92 & 3 & 2.97 & 0.57 & 4.76 & accept \\
        Among Lions (B) & 3.5 & 2 & 1.75 & 0.34 & 5.14 & accept \\
        Region X Lions (A$\times$B) & 31.22 & 6 & 5.20 & & & \\
        Total & 43.64 & 11 & & & & \\
        \bottomrule
    \end{tabular}
\end{table}

\vspace{1cm}

\textbf{\caption{Types of Lions Per Region:}}
\begin{table}[h!]
    \centering
    \begin{tabular}{l c c c c c c}
        \toprule
        Types of Lions & L & M & N & O & Row Total \\
        \midrule
        I & 5 & 8 & 3 & 2 & 18 \\
        II & 1 & 4 & 9 & 8 & 22 \\
        III & 7 & 6 & 3 & 1 & 17 \\
        \midrule
        Column total & 13 & 18 & 15 & 11 & 57 \\
        \bottomrule
    \end{tabular}
\end{table}

\vspace{1cm}

\noindent Calculations for Types of Lions per Region:
\begin{align*}
    \text{Among A} \implies \frac{13^2 + 18^2 + 15^2 + 11^2}{3} - \frac{(57)^2}{12} \\
    &= 279.67 - 270.75 \\
    &= 8.92
\end{align*}

\begin{align*}
    \text{Among B} \implies \frac{18^2 + 22^2 + 17^2}{4} - 270.75 \\
    &= 3.5
\end{align*}

\newpage

\section*{Paired $t$-test}

\qquad

*Compares two related or matched groups.

\qquad

\begin{tabular}{lccc}
\toprule
\textbf{Class Grades} & \textbf{Before class} & \textbf{After class} & \textbf{Difference} \\
\midrule
I & 8 & 10 & -2 \\
II & 5 & 10 & -5 \\
III & 3 & 9 & -6 \\
IV & 8 & 8 & 0 \\
V & 6 & 5 & 1 \\
\midrule
Total & 30 & 42 & 12 &&& *absolute value for difference\\
\bottomrule
\end{tabular}

\vspace{1cm}

$$\therefore t_c = \frac{\bar{D}}{s_{\bar{D}}}$$

$$\therefore \bar{D} = \frac{12}{5} = 2.4$$

s_{\bar{D}} = $$s^2 = \frac{\sum (D)^2 - (\sum D)^2/n}{n-1} = \frac{66 - (144)/5}{4} = 9.3$$

$$\therefore s_{\bar{D}} = \sqrt{\frac{9.3}{n}} = \sqrt{\frac{9.3}{5}} = 1.36$$

\qquad

\noindent $s^2$ is the sample variance of a sum of different scores where $t$ is the test statistic, $\bar{D}$ is the mean of the difference between paired observations and $s_{\bar{D}}$ is the standard error of the mean difference.


\newpage

So. $t_{c}=\frac{\overline{D}}{S_{\overline{D}}}$
$$=\frac{2.4}{1.36}=1.76.$$
So $t_{0.05(4)} = 2.776$

\qquad

where \alpha is 0.05 

\qquad

$t_{c} < t_{\text{table}}$ so accept...

\vspace{1cm}

\newpage

\section*{Homogeneity of Group Variances}

Measured using $F_{\text{max}}$.

\qquad

It ensures that there is a fair comparison between each group and the variances within each group are approximately equal.

\qquad


Formula: 

\qquad

$F_{\text{max}} = \frac{\text{Largest } S^2}{\text{Smallest } S^2}$

\qquad

where $s^2$ is the sample variance 

\qquad

Homogeniety of Group Variance Example Table: 

\qquad

\begin{tabular}{@{}lcccc@{}}
\toprule
& w & x & y & z \\
\midrule
& 8 & 10 & 2 & 8 \\
& 5 & 10 & 5 & 3 \\
& 3 & 9 & 6 & 2 \\
& 8 & 8 & 0 & 0 \\
& 6 & 5 & 1 & 1 \\
\midrule
Total $\Rightarrow$ & 30 & 42 & 14 & 14 \\
mean $\Rightarrow$ & 6 & 8.4 & 2.8 & 2.8 \\
$S^2 \Rightarrow$ & 4.5 & 4.3 & 6.7 & 9.7 \\
\bottomrule
\end{tabular}

\newpage

So: $F_{\text{max}_{\text{cal}}} = \frac{9.7}{4.3} = 2.26$.

\qquad

So: $F_{\text{max}_{\text{table}}} = 20.6$.

\qquad

$F_{\text{max}_{\text{cal}}} < F_{\text{max}_{\text{table}}} \Rightarrow \text{accept } H_0$.

\vspace{1cm}

\newpage

\noindent \textbf{Non-Parametric Tests:}

\qquad


\noindent \textbf{Mann-Whitney ($U$ test)} - is an alternative to the independent t-test. It is used for two independent groups. 

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\omega$ & $x$ & ordered($\omega$) & ordered ($x$) & Ranked ($\omega$) \\
\hline
8 & 10 & 3 & 5 & 0 \\
5 & 10 & 5 & 8 & 0.5 \\
3 & 9  & 6 & 9 & 1 \\
8 & 8  & 8 & 10 & 1.5 \\
6 & 5  & 8 & 10 & 1.5\\
\hline
\multicolumn{5}{r}{$\Sigma R = 4.5$} & \\
\end{tabular}
\end{center}

$C = \Sigma R_n = 4.5$.

\qquad

$C' = n_1 n_2 - C$

\qquad

$C' = (5)(5) - 4.5 = 25 - 4.5 = 20.5$

\qquad

$U_{(5, 5)} = 21$

\qquad

Since $C$ or $C'$ is smaller than $U$ then accept.

\qquad

\newpage

\section*{Wilcoxon:} - Another alternative to the independent t-test and is used for two dependent groups (paired). 

\qquad

\centering

\begin{tabular}{c c c c} 
w & x & ordered (w) & ordered (x) \\
\hline
8 & 10 & 3 & 5 \\
5 & 10 & 5 & 8 \\
3 & 9 & 6 & 9\\
8 & 8 & 8 & 10   \\
6 & 5 & 8 & 10\\
\end{tabular}
\hfill
\begin{tabular}{c c}

\end{tabular}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{Screen Shot 2025-11-20 at 8.17.17 PM.png}
    \caption{ Wilcoxon ranking test for two independent groups from table above}
    \label{fig:}
\end{figure}

\vspace{1cm}

\qquad



\noindent
\begin{tabular}{c c}
Rank (w) & Rank (x) \\
\hline
1 & 2.5 \\
2.5 & 6 \\
4 & 8 \\
6 & 9.5 \\
6 & 9.5 \\
Total 19.5 & 35.5 \\
\end{tabular}

\vspace{1cm}

\noindent
\begin{minipage}[t]{0.6\textwidth}
\begin{align*}
C' &= n_1 n_2 + \frac{n_1(n_1+1)}{2} - \Sigma R_1 \\
$$C = n_1 n_2 - C_1$$
&= (5)(5) + \frac{(5)(5+1)}{2} - 19.5 \\
&= 25 + \frac{30}{2} - 19.5 \\
&= 20.5
\end{align*}
\end{minipage}


\vspace{1cm}

\noindent
\begin{align*}
C' &= (5)(5) - 20.5 \\
&= 25 - 20.5 \\
&= 4.5
\end{align*}

\newpage

\section*{Kruskal-Wallis H-test. continuous data}

\qquad

Yet another non-parametric test, but While Mann-Whitney and Wilcoxin are t-test alternatives, the Kruskal-Wallis H-test is an Anova alternative. 

\qquad

The H-test is used to determine if the medians of three or more independent 
groups differ. 

Example below: 

\qquad

$$H_{c}=\frac{12}{N(N+1)}\times\left(\frac{\Sigma R^{2}}{N_{i}}\right)-3(N+1)$$

\vspace{0.5cm}

\begin{tabular}{c|ccc}
\textbf{x} & \textbf{y} & \textbf{z} \\
\hline
8 & 6 & 8 \\
9 & 2 & 9 \\
5 & 3 & 7 \\
3 & 4 & 5 \\
\end{tabular}

\qquad

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{Screen Shot 2025-11-20 at 8.20.51 PM.png}
    \caption{Ranking of samples in Kruskal-Wallis H-test}
    \label{fig:placeholder}
\end{figure}

\qquad

\begin{tabular}{c|ccc}
\textbf{Rank} & \textbf{x} & \textbf{y} & \textbf{z} \\
\hline
9.5 & 7 & 9.5 \\
11.5 & 1 & 11.5 \\
5.5 & 2.5 & 8 \\
2.5 & 4 & 5.5 \\
\hline
29 & 14.5 & 34.5 \\
\end{tabular}

\vspace{0.5cm}

$$H_{c}=\frac{12}{12(12+1)}\times\left(\frac{29^{2}+14.5^{2}+(34.5)^{2}}{4}\right)-3(12+1)$$

$$H_{c}=\frac{12}{156}\left(\frac{2241.5}{4}\right)-39 = 4.11$$

\vspace{1cm}

$$\chi^{2}_{0.05(a-1)}=\chi^{2}_{0.05(2)}=5.991$$

\noindent So: $H_{c}<\chi^{2}$ accept $H_{0}$.

\newpage

\section*{Regression Analysis}

\qquad

Regression analysis help determine the relationship between a dependent variable and two or more independent variables. 

\qquad

We can use the values from the table below to examine regression.

\qquad

\begin{tabular}{|c|c|c|c|c|}
\hline
$x$ & $y$ & $xy$ & $x^2$ & $y^2$ \\
Diabetes & Death & & & \\
\hline
0 & 5 & 0 & 0 & 25 \\
2 & 3 & 6 & 4 & 9 \\
4 & 2 & 8 & 16 & 4 \\
6 & 8 & 48 & 36 & 64 \\
\hline
12 & 18 & 62 & 56 & 102 \\
\hline
\end{tabular}

\vspace{1cm}

recall $\Rightarrow y = mx + c$.

\qquad

Regression coefficient / slope factor. -solving for the slope (m)

$$m = \frac{\sum xy - ((\sum x) (\sum y)/n)}{\sum x^2 - (\sum x)^2/n}$$

$$m = \frac{62 - \{ (12)(18)/4 \}}{56 - 12^2/4}$$

$$= \frac{62 - 54}{56 - 144/4}$$

$$= \frac{8}{20}$$

$$= 0.4$$

\newpage

\section*{Regression Analysis continued...}

To test that the population regression coefficient was zero, use $\beta=0$, use Anova.

\qquad

The β (beta) coefficients represent the slope(s)-the change in the dependent variable Y for a one-unit change in the predictor X.

\begin{center}
\begin{tabular}{cccccc}
\toprule
Source of Variance & SS & Df & MS & $F_s$ & $F_{0.05}$ (1,2) Decision \\
\midrule
Explained (due to known regression) & 3.2 & 1 & 3.2 & 0.36 & 18.5 accept \\
Unexplained (due to non-known linear regression) & 17.8 & $n-2=2$ & 8.9 & & \\
Total ss & 21 & $n-1=3$ & 7 & & \\
\bottomrule
\end{tabular}
\end{center}

Explained $SS = \frac{Sp^2}{SS_x}$

\qquad

$Sp^2$ is the squared sum of products.

\qquad

$\frac{Sp^2}{SS_x} = \frac{8^2}{20} = \frac{64}{20} = 3.2$.

\qquad

Total ss $= SS_y = \sum y^2 - \frac{(\sum y)^2}{n}.$

\qquad

$= 102 - \frac{(18)^2}{4}$

\qquad

$= 102 - 81$

\qquad

$= 21$

\qquad

Calculating the explained variation:

\qquad

$$\frac{\text{Explained SS}}{\text{total SS}} = \frac{3.2}{24.2} = 0.132$$

\qquad

So Since $y=mx+c$, the predicted death rate if 7 people had diabetes.
Use $\bar{y}$ \& $\bar{x}$ $\implies \bar{y}=m\bar{x}+c$.
$$\bar{y}=\frac{18}{4}=4.5$$
$$\bar{x}=\frac{12}{4}=3$$
$$4.5=0.4(3)+c$$
$$4.5-1.2=c$$
$$c=3.3$$
$$\therefore y=0.4x+3.3$$
So, if predicted diabetes rate was 7.
$$\therefore y=(0.4)(7)+3.3$$
$$y=6.1 \implies \text{death}=6.1$$

would die at that rate of diabetes. 

\qquad



\newpage

\section*{Correlation Analysis}

$+$ In Correlation Analysis,  both variables are random and the relationship between their strength and direction is determined as a result. 

\vspace{1em}

So:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$Y_1$ & $Y_2$ & $Y_1Y_2$ & $Y_1^2$ & $Y_2^2$ \\
\hline
5 & 6 & 30 & 25 & 36 \\
8 & 2 & 16 & 64 & 4 \\
3 & 5 & 15 & 9 & 25 \\
2 & 8 & 16 & 4 & 64 \\
\hline
Total: 18 & 21 & 77 & 102 & 129 \\
\hline
\end{tabular}
\end{center}

\vspace{1em}

$r \Rightarrow$ correlation coefficient

$r^2 or R^2 \Rightarrow$ coefficient of determination. $\Rightarrow$ explained variation

\qquad

$1-r^2 \Rightarrow$ coefficient of non-determination./unexplained variation

\qquad

$\sqrt{1-r^2} \Rightarrow$ coefficient of Alienation/disparity between two variables.

\qquad

To calculate coefficient of correlation: 

\vspace{1em}

$r = \frac{\text{sum of products}}{\sqrt{(SS_{Y_1})(SS_{Y_2})}} = \frac{\sum Y_1Y_2 - \frac{(\sum Y_1)(\sum Y_2)}{n}}{\sqrt{\left(\sum Y_1^2 - \frac{(\sum Y_1)^2}{n}\right)\left(\sum Y_2^2 - \frac{(\sum Y_2)^2}{n}\right)}}$

\vspace{1em}

To Find $r \Rightarrow \frac{77 - \frac{(18)(21)}{4}}{\sqrt{\left(102 - \frac{(18)^2}{4}\right)\left(129 - \frac{(21)^2}{4}\right)}} = \frac{-17.5}{\sqrt{(21)(18.75)}} = \mathbf{-0.88}$

\newpage

$r^2 = (-0.88)$

$= 0.77$

$1-r^2 = 0.23$

$\sqrt{1-r^2} = \sqrt{0.23}$

\qquad

$= 0.48$
is the coefficient of non determination. 

\qquad

Testing the hypothesis that there is no relationship between the two variables in question ($y_1$, $y_2$)

\qquad

$\therefore P=0$ \& $P \neq 0$

\qquad

Test statistic for $\beta \Rightarrow t = \frac{r}{r_{SE}}$

$r_{SE} = \sqrt{\frac{1-r^2}{n-2}}$

\qquad

$t_u = \frac{r}{\sqrt{\frac{1-r^2}{n-2}}} = \frac{-0.88}{\sqrt{\frac{0.23}{4-2}}}$

\qquad

$= -2.595$

\qquad

$t_{0.05}(n=4) = 2.776$

\qquad

$\Rightarrow t_{cal} < t_{table} \Rightarrow$ accept $H_0$.

\qquad

So, there is no significant difference between the two variables. 

\newpage

\section*{Chi-Square Goodness of Fit (Mendelian Genetics)}

\qquad

Chi square test whether there is any significant difference between the observed and expected. 

\qquad

Table showing punnet square for dihybrid genetic cross: 
\begin{figure}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{} & \textbf{AA} & \textbf{Aa} & \textbf{aA} & \textbf{aa} \\
        \hline
        \textbf{BB} & ABAB & ABaB & aBAB & aBaB \\
        \hline
        \textbf{Bb} & ABAb & ABab & ABab & aBab \\
        \hline
        \textbf{bB} & ABAb & ABab & ABab & aBab \\
        \hline
        \textbf{bb} & AbAb & Abab & Abab & abab \\
        \hline
    \end{tabular}
    \caption{Punnett square for a dihybrid cross (handwritten interpretation)}
\end{figure}

Phenotypic ratio $\Rightarrow 9:3:3:1$

\qquad

These are the expected values, so the ratios are as follow below:

\qquad

Ratios: $\frac{9}{16}:\frac{3}{16}:\frac{3}{16}:\frac{1}{16}$

\qquad

If Observed is as below:
\begin{itemize}
    \item AB = 32
    \item aB = 12
    \item Ab = 12
    \item ab = 3
\end{itemize}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{} & \textbf{observed (O)} & \textbf{Expected (E)} & \textbf{O - E} & \textbf{$(O-E)^2$} & \textbf{$\frac{(O-E)^2}{E}$} \\
        \hline
        AB & 32 & 33.19 & -1.19 & 1.42 & 0.04 \\
        aB & 12 & 11.06 & 0.94 & 0.88 & 0.08 \\
        Ab & 12 & 11.06 & 0.94 & 0.88 & 0.08 \\
        ab & 3 & 3.69 & -0.69 & 0.48 & 0.13 \\
        \hline
        \textbf{Total} & 59 & & & & 0.21 \\
        \hline
    \end{tabular}
    \caption{Chi-square goodness of fit table}
\end{table}

\newpage

\qquad

To normalize our expected values based on the ratios for our punnet square: 

% Calculations
$$\frac{9}{16} \times \frac{59}{1} = 33.19$$
$$\frac{3}{16} \times \frac{59}{1} = 11.06$$
$$\frac{1}{16} \times \frac{59}{1} = 3.69$$

% Chi-squared test
$$\chi^2_{0.05(n-1)} 
% Assuming n-1 = 3 from context
$$\chi^2_{0.05(3)} \Rightarrow 7.815$$

\qquad

Where n=4 for amount of samples and types. 


$$\text{0.21} < 7.815 \Rightarrow \text{accept } H_0$$

\qquad

where a threshold of 0.05 is used. 
$$\chi^2_{\text{cal}} < \chi^2_{\text{table}}$$

\vspace{1cm} % Add some vertical space

\newpage

% Independence / Contingency table
\section*{Independence / Contingency table}

\qquad

The Independence table displays the frequency distribution for two categorical variables and determine if they are independent or associated based on their relationship as seen in the table below: 

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Drug} & \textbf{worked} & \textbf{did not work} & \textbf{Total} \\
        \hline
        Drug (A) & 30 & 15 & 45 \\
        no drug (B) & 18 & 23 & 41 \\
        \hline
        \textbf{Total} & 48 & 38 & 86 \\
        \hline
    \end{tabular}
    \caption{Contingency table data}
\end{table}

\textbf{So}, based on the independence contigency table: 

\section*{Expected}

\raggedright

A: (worked + Drug) $\Rightarrow \frac{(48)(45)}{86} = 25.12$

\qquad

B: (Did not work + Drug) $\Rightarrow \frac{(38)(45)}{86} = 19.88$

\qquad

C: (worked + No Drug) $\Rightarrow \frac{(48)(41)}{86} = 22.88$

\qquad

D: (Did not work + No drug) $\Rightarrow \frac{(38)(41)}{86} = 18.12$

\vspace{1cm}

\centering

\noindent
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Observed & Expected & Obs-Exp & $(\text{Obs-Exp})^2$ & $\frac{(\text{Obs-Exp})^2}{\text{Exp}}$ & Decision \\
\hline
30 & 25.12 & 4.88 & 23.81 & 0.95 & accept \\
15 & 19.88 & -4.88 & 23.81 & 1.20 & accept \\
18 & 22.88 & -4.88 & 23.81 & 1.04 & accept \\
23 & 18.12 & 4.88 & 23.81 & 1.31 & accept \\
\hline
Total & & & & 4.5 & \\
\hline
\end{tabular}

\vspace{1cm}


\newpage

\section*{Relative Risk}

\qquad

Relative Risk, Would be used if we want to determine the probability of an event in an exposed group to the probability of the same event in a non-exposed group. 

\qquad

To test correlation using relative risk:

Let's use our drug table from the Independence/ Contingency. 

$$RR=\left(\frac{a}{a+b}\right)\cdot\left(\frac{c+d}{c}\right)$$

$$RR=\left(\frac{30}{30+15}\right)\left(\frac{18+23}{18}\right)$$

$$=\left(\frac{30}{45}\right)\left(\frac{41}{18}\right)$$

$$= 1.52$$

To test correlation using odds ratio:

$$OR=\frac{ad}{bc}$$

$$=\frac{(30)(23)}{(15)(18)}$$

$$=\frac{690}{270}$$

$$= 2.55$$
is the amount of times the risk of an outcome of the drug working (2.55-1) * 100

= 155 \%

\newpage

\section*{What about principal component analysis (PCA)?}

\qquad

This method converts many correlated variables (as seen in a variety linear regression graphs with scatter plots) to a smaller number of uncorrelated variables, that are called principle components. 

\qquad
\begin{figure}[h]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        width=12cm, 
        height=8cm,
        xlabel={Hair (cm)},
        ylabel={Size (cm)},
        axis lines=middle,
        grid=both,
        grid style={dashed, gray!30},
        enlargelimits=0.2,
        title={PCA Scatter Plot with PC1 and PC2},
        legend pos=north west
    ]

    % Data points (Hair, Size)
    \addplot[
        only marks,
        mark=*,
        mark options={fill=blue}
    ] coordinates {
        (20,50)
        (25,55)
        (15,48)
        (30,60)
    };

    % PC1 line (example direction)
    \addplot[
        color=red,
        thick
    ] coordinates {
        (10,40)
        (35,65)
    };

    % PC2 line (perpendicular)
    \addplot[
        color=green,
        thick,
        dashed
    ] coordinates {
        (10,66)
        (35,39)
    };

    

    \end{axis}
    \end{tikzpicture}
    \caption{PCA graph showing data points and the first two principal components (PC1 and PC2). The variables are hair and size- The PCA graph shows, a visualization of how two data dimensions ("Hair" and "Size") can be represented in a new, transformed coordinate system that highlights the primary directions of variance- Blue= hair and size, red= longest axis and green= direction of maximum variance}
    
\end{figure}


\end{document}


